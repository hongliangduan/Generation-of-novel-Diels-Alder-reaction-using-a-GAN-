{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "data_path = 'train, valid and test csv files path'\n",
    "save_path = 'pickle data save path'\n",
    "\n",
    "encode_dict = {\"Br\": 'Y', \"Cl\": 'X', \"Si\": 'A', 'Se': 'Z', '@@': 'R', 'se': 'E'}\n",
    "decode_dict = {v: k for k, v in encode_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition for making data to be used for SMILES-MaskGAN\n",
    "def get_pair(tokens, mask_idxs, mask_id):\n",
    "    idxs = [vocab[atom] for atom in tokens]\n",
    "\n",
    "    def _pad(ls, pad_index, max_length=100):\n",
    "        padded_ls = deepcopy(ls)\n",
    "\n",
    "        while len(padded_ls) <= max_length:\n",
    "            padded_ls.append(pad_index)\n",
    "\n",
    "        return padded_ls\n",
    "\n",
    "    srcs = deepcopy(idxs)\n",
    "    srcs.append(vocab['<eos>'])  # append eos id in srcs last\n",
    "\n",
    "    tgts = deepcopy(idxs)\n",
    "    tgts.insert(0, vocab['<eos>'])  # insert eos id in tgts first\n",
    "\n",
    "    srcs_pad = _pad(srcs, vocab['<pad>'], max_length=100)\n",
    "    tgts_pad = _pad(tgts, vocab['<pad>'], max_length=100)\n",
    "\n",
    "    mask = torch.zeros(len(tgts_pad))\n",
    "    for mask_idx in mask_idxs:\n",
    "        offset = 1\n",
    "        mask[mask_idx + offset] = 1\n",
    "        srcs[mask_idx] = mask_id\n",
    "\n",
    "    return srcs_pad, tgts_pad, len(srcs), mask\n",
    "\n",
    "\n",
    "def encode(smiles: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace multi-char tokens with single tokens in SMILES string.\n",
    "\n",
    "    Args:\n",
    "        smiles: SMILES string\n",
    "\n",
    "    Returns:\n",
    "        sanitized SMILE string with only single-char tokens\n",
    "    \"\"\"\n",
    "\n",
    "    temp_smiles = smiles\n",
    "    for symbol, token in encode_dict.items():\n",
    "        temp_smiles = temp_smiles.replace(symbol, token)\n",
    "    return temp_smiles\n",
    "\n",
    "\n",
    "class Mask:\n",
    "    mask_token = '__<m>__'\n",
    "\n",
    "    def __call__(self, n):\n",
    "        idxs = self.forward(n)\n",
    "\n",
    "        # Verify indices are okay.\n",
    "        assert (len(idxs) < n)\n",
    "\n",
    "        valid_set = set(list(range(n)))\n",
    "        for i in idxs:\n",
    "            assert (i in valid_set)\n",
    "\n",
    "        return idxs\n",
    "    \n",
    "\n",
    "class StochasticMask(Mask):\n",
    "    def __init__(self, probability):\n",
    "        self.p = probability\n",
    "        self.r = random.Random(42)\n",
    "\n",
    "    def forward(self, n):\n",
    "        # Starting from one, since masks are messed,\n",
    "        k = int(n * self.p)\n",
    "        idxs = self.r.sample(range(1, n), k)\n",
    "\n",
    "        return idxs\n",
    "    \n",
    "\n",
    "class VocabBuilder:\n",
    "    def __init__(self, mask_builder):\n",
    "        self.vocab_path = os.path.join(data_path, 'vocab', 'vocab.pt')\n",
    "\n",
    "        self.mask_builder = mask_builder\n",
    "        self._vocab = None\n",
    "\n",
    "    def vocab(self):\n",
    "        if self._vocab is None:\n",
    "            self.build_vocab()\n",
    "\n",
    "        return self._vocab\n",
    "\n",
    "    def build_vocab(self):\n",
    "        if os.path.exists(self.vocab_path):\n",
    "            self._vocab = torch.load(self.vocab_path)\n",
    "\n",
    "        else:\n",
    "            self.rebuild_vocab()\n",
    "\n",
    "    def rebuild_vocab(self):\n",
    "        self.forbidden_symbols = {'Ag', 'Al', 'Am', 'Ar', 'At', 'Au', 'D', 'E', 'Fe', 'G', 'K', 'L', 'M', 'Ra', 'Re',\n",
    "                                  'Rf', 'Rg', 'Rh', 'Ru', 'T', 'U', 'V', 'W', 'Xe',\n",
    "                                  'Y', 'Zr', 'a', 'd', 'f', 'g', 'h', 'k', 'm', 'si', 't', 'te', 'u', 'v', 'y'}\n",
    "\n",
    "        self._vocab = {'<unk>': 0, '<pad>': 1, '<eos>': 2, '#': 20, '%': 22, '(': 25, ')': 24, '+': 26, '-': 27,\n",
    "                         '.': 30,\n",
    "                         '0': 32, '1': 31, '2': 34, '3': 33, '4': 36, '5': 35, '6': 38, '7': 37, '8': 40,\n",
    "                         '9': 39, '=': 41, 'A': 7, 'B': 11, 'C': 19, 'F': 4, 'H': 6, 'I': 5, 'N': 10,\n",
    "                         'O': 9, 'P': 12, 'S': 13, 'X': 15, 'Y': 14, 'Z': 3, '[': 16, ']': 18,\n",
    "                         'b': 21, 'c': 8, 'n': 17, 'o': 29, 'p': 23, 's': 28,\n",
    "                         \"@\": 42, \"R\": 43, '/': 44, \"\\\\\": 45, 'E': 46, '__<m>__': 47\n",
    "                         }\n",
    "\n",
    "        self.idx_char = {v: k for k, v in self._vocab.items()}\n",
    "\n",
    "        if self.vocab_path is not None:\n",
    "            torch.save(self._vocab, self.vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = os.path.join(data_path, 'data.csv')  # change data name\n",
    "data = pd.read_csv(dir_)\n",
    "smiles_list = [line.strip() for line in data['canonical_smiles']]\n",
    "encoded_list = [encode(line) for line in smiles_list if len(line) <= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmask = StochasticMask(0.1)\n",
    "\n",
    "vocab = None\n",
    "if vocab is None:\n",
    "    builder = VocabBuilder(rmask)\n",
    "    vocab = builder.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9efff0162d446ebef45e66200dfef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=837136.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = defaultdict(\n",
    "    list, {\n",
    "        k: [] for k in ('train_srcs', 'train_tgts', 'train_lengths', 'train_mask')})\n",
    "\n",
    "with tqdm(total=len(encoded_list)) as tbar:\n",
    "    for i, tokens in enumerate(encoded_list):\n",
    "        seq_len = len(tokens)\n",
    "        mask_idxs = rmask(seq_len)\n",
    "        mask_id = vocab['__<m>__']\n",
    "\n",
    "        src, tgt, length, mask = get_pair(tokens, mask_idxs, mask_id)\n",
    "        final_data['train_srcs'].append(src)\n",
    "        final_data['train_tgts'].append(tgt)\n",
    "        final_data['train_lengths'].append(length)\n",
    "        final_data['train_mask'].append(mask)\n",
    "        \n",
    "\n",
    "        tbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pickle file\n",
    "with open(os.path.join(data_path, 'chembl26_canon_train_0.1.pkl'),'wb') as f:\n",
    "    pickle.dump(final_data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle file\n",
    "with open(os.path.join(data_path, 'chembl26_canon_train_0.1.pkl'),'rb') as f:\n",
    "    val_data = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
